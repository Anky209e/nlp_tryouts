{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import string\n",
    "import numpy\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "all_chars = string.printable\n",
    "n_characters = len(all_chars)\n",
    "\n",
    "file = unidecode.unidecode(open(\"robert_frost.txt\").read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_layers,output_size):\n",
    "        super(RNN,self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # Input_size is max no of characters hidden size is dimension of embedding\n",
    "        self.embed = nn.Embedding(input_size,hidden_size) \n",
    "        self.lstm = nn.LSTM(hidden_size,hidden_size,num_layers,batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size,output_size)\n",
    "    \n",
    "    def forward(self,x,hidden,cell):\n",
    "\n",
    "        out = self.embed(x)\n",
    "        out,(hidden,cell) = self.lstm(out.unsqueeze(1),(hidden,cell))\n",
    "        out = self.fc(out.reshape(out.shape[0],-1))\n",
    "\n",
    "        return out,(hidden,cell)\n",
    "    \n",
    "    def init_hidden(self,batch_size):\n",
    "\n",
    "        hidden = torch.zeros(self.num_layers,batch_size,self.hidden_size).to(device)\n",
    "        cell = torch.zeros(self.num_layers,batch_size,self.hidden_size).to(device)\n",
    "\n",
    "        return hidden,cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator():\n",
    "    def __init__(self):\n",
    "        self.chunk_len = \n",
    "        self.num_epoch = 2000\n",
    "        self.batch_size = 1\n",
    "        self.print_every = 2\n",
    "        self.hidden_size = 300\n",
    "        self.num_layers = 3\n",
    "        self.lr = 0.003\n",
    "        self.rnn = RNN(n_characters,self.hidden_size,self.num_layers,n_characters).to(device)\n",
    "\n",
    "    def char_tensor(self,string):\n",
    "        tensor = torch.zeros(len(string)).long()\n",
    "        for c in range(len(string)):\n",
    "            # Replacing zero with character index\n",
    "            tensor[c] = all_chars.index(string[c])\n",
    "        return tensor\n",
    "    \n",
    "    def get_random_batch(self):\n",
    "        # start index of our random batch\n",
    "        start_idx = random.randint(0,len(file)-self.chunk_len)\n",
    "        # end index of our random batch\n",
    "        end_idx = start_idx + self.chunk_len + 1\n",
    "        # picking text string from file using start and end index\n",
    "        text_str = file[start_idx:end_idx]\n",
    "        text_input = torch.zeros(self.batch_size,self.chunk_len)\n",
    "        text_target = torch.zeros(self.batch_size,self.chunk_len)\n",
    "\n",
    "        # Targets will be next character in sequence since we will be predicting next characters\n",
    "        for i in range(self.batch_size):\n",
    "            text_input[i,:] = self.char_tensor(text_str[:-1])\n",
    "            text_target[i,:] = self.char_tensor(text_str[1:])\n",
    "        \n",
    "        return text_input.long(),text_target.long()\n",
    "\n",
    "    def generate(self,initial_str='A',prediction_len=1000,temprature=0.85):\n",
    "        hidden,cell = self.rnn.init_hidden(self.batch_size)\n",
    "        initial_input = self.char_tensor(initial_str)\n",
    "        predicted = initial_str\n",
    "        \n",
    "        # Handeling LSTM hidden and cell if initial string is longer than 1\n",
    "        for p in range(len(initial_str)-1):\n",
    "            _,(hidden,cell) = self.rnn(initial_input[p].view(1).to(device),hidden,cell)\n",
    "\n",
    "        last_char = initial_input[-1]\n",
    "\n",
    "        for p in range(prediction_len):\n",
    "            output,(hidden,cell) = self.rnn(last_char.view(1).to(device),hidden,cell)\n",
    "            # Handeling temperature for predictions\n",
    "            output_dist = output.data.view(-1).div(temprature).exp()\n",
    "            # Finding characters with highest probs but with a little randomisation too\n",
    "            top_char_probs = torch.multinomial(output_dist,1)\n",
    "            top_char = top_char_probs[0]\n",
    "            # print(top_char)\n",
    "            predicted_char = all_chars[top_char]\n",
    "            predicted+=predicted_char\n",
    "\n",
    "            last_char = self.char_tensor(predicted_char)\n",
    "        \n",
    "        return predicted\n",
    "    \n",
    "    def save_weights(self,filename):\n",
    "        torch.save(self.rnn.state_dict(),filename)\n",
    "    \n",
    "    def load_weights(self,filename):\n",
    "        self.rnn.load_state_dict(torch.load(filename))\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        self.rnn = RNN(n_characters,self.hidden_size,self.num_layers,n_characters).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.rnn.parameters(),lr=self.lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        print(\"==> Starting Training\")\n",
    "\n",
    "        for epoch in range(1,self.num_epoch+1):\n",
    "            input,target = self.get_random_batch()\n",
    "\n",
    "            hidden,cell = self.rnn.init_hidden(self.batch_size)\n",
    "\n",
    "            self.rnn.zero_grad()\n",
    "            loss = 0\n",
    "            input = input.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # providing characters 1 by 1\n",
    "            for c in range(self.chunk_len):\n",
    "                output,(hidden,cell) = self.rnn(input[:,c],hidden,cell)\n",
    "                loss += criterion(output,target[:,c])\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss = loss.item()/self.chunk_len\n",
    "\n",
    "            if epoch % self.print_every == 0:\n",
    "                print(f\"===> Epoch:{epoch} ==> Loss:{loss}\")\n",
    "                f_chars = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "                initial_letter = f_chars[random.randint(0,len(f_chars)-1)]\n",
    "                print(f\"-----\\n{self.generate(initial_str=initial_letter)}\\n-----\")\n",
    "        self.save_weights(f\"robert_data_weights_{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model_1 = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for RNN:\n\tMissing key(s) in state_dict: \"lstm.weight_ih_l2\", \"lstm.weight_hh_l2\", \"lstm.bias_ih_l2\", \"lstm.bias_hh_l2\". \n\tsize mismatch for embed.weight: copying a param with shape torch.Size([100, 120]) from checkpoint, the shape in current model is torch.Size([100, 300]).\n\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([480, 120]) from checkpoint, the shape in current model is torch.Size([1200, 300]).\n\tsize mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([480, 120]) from checkpoint, the shape in current model is torch.Size([1200, 300]).\n\tsize mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([480]) from checkpoint, the shape in current model is torch.Size([1200]).\n\tsize mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([480]) from checkpoint, the shape in current model is torch.Size([1200]).\n\tsize mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([480, 120]) from checkpoint, the shape in current model is torch.Size([1200, 300]).\n\tsize mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([480, 120]) from checkpoint, the shape in current model is torch.Size([1200, 300]).\n\tsize mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([480]) from checkpoint, the shape in current model is torch.Size([1200]).\n\tsize mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([480]) from checkpoint, the shape in current model is torch.Size([1200]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([100, 120]) from checkpoint, the shape in current model is torch.Size([100, 300]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gen_model_1\u001b[39m.\u001b[39;49mload_weights(\u001b[39m'\u001b[39;49m\u001b[39mrobert_frost_weights_2000.pth\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[4], line 66\u001b[0m, in \u001b[0;36mGenerator.load_weights\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_weights\u001b[39m(\u001b[39mself\u001b[39m,filename):\n\u001b[0;32m---> 66\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(filename))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1671\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   1667\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1668\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1670\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1671\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1672\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1673\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for RNN:\n\tMissing key(s) in state_dict: \"lstm.weight_ih_l2\", \"lstm.weight_hh_l2\", \"lstm.bias_ih_l2\", \"lstm.bias_hh_l2\". \n\tsize mismatch for embed.weight: copying a param with shape torch.Size([100, 120]) from checkpoint, the shape in current model is torch.Size([100, 300]).\n\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([480, 120]) from checkpoint, the shape in current model is torch.Size([1200, 300]).\n\tsize mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([480, 120]) from checkpoint, the shape in current model is torch.Size([1200, 300]).\n\tsize mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([480]) from checkpoint, the shape in current model is torch.Size([1200]).\n\tsize mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([480]) from checkpoint, the shape in current model is torch.Size([1200]).\n\tsize mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([480, 120]) from checkpoint, the shape in current model is torch.Size([1200, 300]).\n\tsize mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([480, 120]) from checkpoint, the shape in current model is torch.Size([1200, 300]).\n\tsize mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([480]) from checkpoint, the shape in current model is torch.Size([1200]).\n\tsize mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([480]) from checkpoint, the shape in current model is torch.Size([1200]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([100, 120]) from checkpoint, the shape in current model is torch.Size([100, 300])."
     ]
    }
   ],
   "source": [
    "gen_model_1.load_weights('robert_frost_weights_2000.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I been locked inside your heart shaped box for weeks to times or the swarm\n",
      "And see make the cellar house that was where the farm- \n",
      "John some house the house the read that was no one of know of the same that I'm old tell them light\n",
      "To mean that he stairs\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"I been locked inside your heart shaped box for weeks \"\n",
    "out_1 = gen_model_1.generate(initial_str=seed_text,prediction_len=100*2,temprature=0.80)\n",
    "print(out_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look at the stars\n",
      "look how they shine for you\n",
      "they were all yellow\n",
      "We lip the tempting for a prese.' \n",
      "'Cold do be new in see she cellar she had from that, \n",
      "But down an attic,\n",
      "The book out of the jest in a use out of a beltess window\n",
      "The left it a back where it rattle\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"Look at the stars\\nlook how they shine for you\\nthey were all yellow\\n\"\n",
    "out_2 = gen_model_1.generate(initial_str=seed_text,prediction_len=100*2,temprature=0.80)\n",
    "print(out_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
