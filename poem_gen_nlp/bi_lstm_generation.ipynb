{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "BOwsuGQQY9OL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-25 14:19:23.491187: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-25 14:19:24.412405: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
            "2023-04-25 14:19:24.412465: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
            "2023-04-25 14:19:24.412473: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow.keras.utils as ku \n",
        "import numpy as np "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "colab_type": "code",
        "id": "buwQfXNhoHHM",
        "outputId": "773b2b28-86fd-4ff7-dd2c-a3704b62a167"
      },
      "outputs": [],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/robert_frost.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "PRnDnCW-Z7qv"
      },
      "outputs": [],
      "source": [
        "data = open('robert_frost.txt').read()\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# create input sequences using list of tokens\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tinput_sequences.append(n_gram_sequence)\n",
        "\n",
        "# pad sequences \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# create predictors and label\n",
        "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "label = ku.to_categorical(label, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "colab_type": "code",
        "id": "w9vH8Y59ajYL",
        "outputId": "344a813b-8aff-44e6-e5ee-eb1ff417b624"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-25 14:19:34.706367: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2023-04-25 14:19:34.706492: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: candyland\n",
            "2023-04-25 14:19:34.706515: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: candyland\n",
            "2023-04-25 14:19:34.706809: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 525.60.11\n",
            "2023-04-25 14:19:34.706879: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 525.60.11\n",
            "2023-04-25 14:19:34.706890: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 525.60.11\n",
            "2023-04-25 14:19:34.707555: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 11, 128)           290944    \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 11, 240)          239040    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 11, 240)           0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 96)                129408    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1136)              110192    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2273)              2584401   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,353,985\n",
            "Trainable params: 3,353,985\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 128, input_length=max_sequence_len - 1))\n",
        "model.add(Bidirectional(LSTM(120, return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(96))\n",
        "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "AIg2f1HBxqof",
        "outputId": "59e8e0ab-7945-44ef-cc9d-a9ecc486265c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "298/298 [==============================] - 25s 69ms/step - loss: 6.8039 - accuracy: 0.0471\n",
            "Epoch 2/100\n",
            "298/298 [==============================] - 17s 56ms/step - loss: 6.2517 - accuracy: 0.0488\n",
            "Epoch 3/100\n",
            "298/298 [==============================] - 14s 46ms/step - loss: 6.0904 - accuracy: 0.0512\n",
            "Epoch 4/100\n",
            "298/298 [==============================] - 22s 75ms/step - loss: 5.9859 - accuracy: 0.0563\n",
            "Epoch 5/100\n",
            "298/298 [==============================] - 19s 65ms/step - loss: 5.8914 - accuracy: 0.0589\n",
            "Epoch 6/100\n",
            "298/298 [==============================] - 20s 66ms/step - loss: 5.7801 - accuracy: 0.0637\n",
            "Epoch 7/100\n",
            "298/298 [==============================] - 19s 65ms/step - loss: 5.6745 - accuracy: 0.0679\n",
            "Epoch 8/100\n",
            "298/298 [==============================] - 20s 67ms/step - loss: 5.5861 - accuracy: 0.0698\n",
            "Epoch 9/100\n",
            "298/298 [==============================] - 14s 46ms/step - loss: 5.5156 - accuracy: 0.0718\n",
            "Epoch 10/100\n",
            "298/298 [==============================] - 13s 44ms/step - loss: 5.4491 - accuracy: 0.0770\n",
            "Epoch 11/100\n",
            "298/298 [==============================] - 13s 44ms/step - loss: 5.3850 - accuracy: 0.0797\n",
            "Epoch 12/100\n",
            "298/298 [==============================] - 13s 44ms/step - loss: 5.3114 - accuracy: 0.0815\n",
            "Epoch 13/100\n",
            "298/298 [==============================] - 13s 44ms/step - loss: 5.2444 - accuracy: 0.0875\n",
            "Epoch 14/100\n",
            "298/298 [==============================] - 13s 44ms/step - loss: 5.1873 - accuracy: 0.0902\n",
            "Epoch 15/100\n",
            "298/298 [==============================] - 13s 44ms/step - loss: 5.1103 - accuracy: 0.0934\n",
            "Epoch 16/100\n",
            "298/298 [==============================] - 13s 44ms/step - loss: 5.0272 - accuracy: 0.1021\n",
            "Epoch 17/100\n",
            "298/298 [==============================] - 13s 44ms/step - loss: 4.9557 - accuracy: 0.1058\n",
            "Epoch 18/100\n",
            "298/298 [==============================] - 13s 44ms/step - loss: 4.8812 - accuracy: 0.1103\n",
            "Epoch 19/100\n",
            "298/298 [==============================] - 13s 44ms/step - loss: 4.8143 - accuracy: 0.1201\n",
            "Epoch 20/100\n",
            "298/298 [==============================] - 13s 44ms/step - loss: 4.7459 - accuracy: 0.1232\n",
            "Epoch 21/100\n",
            "298/298 [==============================] - 13s 44ms/step - loss: 4.6777 - accuracy: 0.1264\n",
            "Epoch 22/100\n",
            "298/298 [==============================] - 13s 45ms/step - loss: 4.6011 - accuracy: 0.1339\n",
            "Epoch 23/100\n",
            "298/298 [==============================] - 14s 45ms/step - loss: 4.5326 - accuracy: 0.1408\n",
            "Epoch 24/100\n",
            "298/298 [==============================] - 15s 49ms/step - loss: 4.4671 - accuracy: 0.1499\n",
            "Epoch 25/100\n",
            "298/298 [==============================] - 23s 76ms/step - loss: 4.3990 - accuracy: 0.1555\n",
            "Epoch 26/100\n",
            "298/298 [==============================] - 21s 69ms/step - loss: 4.3228 - accuracy: 0.1641\n",
            "Epoch 27/100\n",
            "298/298 [==============================] - 20s 67ms/step - loss: 4.2535 - accuracy: 0.1724\n",
            "Epoch 28/100\n",
            "298/298 [==============================] - 21s 69ms/step - loss: 4.1815 - accuracy: 0.1822\n",
            "Epoch 29/100\n",
            "298/298 [==============================] - 21s 69ms/step - loss: 4.1190 - accuracy: 0.1842\n",
            "Epoch 30/100\n",
            "298/298 [==============================] - 20s 67ms/step - loss: 4.0391 - accuracy: 0.2002\n",
            "Epoch 31/100\n",
            "298/298 [==============================] - 20s 66ms/step - loss: 3.9799 - accuracy: 0.2057\n",
            "Epoch 32/100\n",
            "298/298 [==============================] - 20s 66ms/step - loss: 3.9096 - accuracy: 0.2140\n",
            "Epoch 33/100\n",
            "298/298 [==============================] - 20s 67ms/step - loss: 3.8361 - accuracy: 0.2265\n",
            "Epoch 34/100\n",
            "298/298 [==============================] - 20s 66ms/step - loss: 3.7744 - accuracy: 0.2347\n",
            "Epoch 35/100\n",
            "298/298 [==============================] - 20s 67ms/step - loss: 3.7067 - accuracy: 0.2479\n",
            "Epoch 36/100\n",
            "298/298 [==============================] - 19s 64ms/step - loss: 3.6392 - accuracy: 0.2560\n",
            "Epoch 37/100\n",
            "298/298 [==============================] - 15s 52ms/step - loss: 3.5931 - accuracy: 0.2686\n",
            "Epoch 38/100\n",
            "298/298 [==============================] - 15s 50ms/step - loss: 3.5194 - accuracy: 0.2839\n",
            "Epoch 39/100\n",
            "298/298 [==============================] - 15s 50ms/step - loss: 3.4645 - accuracy: 0.2933\n",
            "Epoch 40/100\n",
            "298/298 [==============================] - 18s 60ms/step - loss: 3.4099 - accuracy: 0.3052\n",
            "Epoch 41/100\n",
            "298/298 [==============================] - 14s 48ms/step - loss: 3.3332 - accuracy: 0.3160\n",
            "Epoch 42/100\n",
            "298/298 [==============================] - 14s 46ms/step - loss: 3.2720 - accuracy: 0.3326\n",
            "Epoch 43/100\n",
            "298/298 [==============================] - 14s 46ms/step - loss: 3.2238 - accuracy: 0.3408\n",
            "Epoch 44/100\n",
            "298/298 [==============================] - 14s 46ms/step - loss: 3.1750 - accuracy: 0.3518\n",
            "Epoch 45/100\n",
            "298/298 [==============================] - 17s 56ms/step - loss: 3.1247 - accuracy: 0.3603\n",
            "Epoch 46/100\n",
            "298/298 [==============================] - 20s 66ms/step - loss: 3.0582 - accuracy: 0.3726\n",
            "Epoch 47/100\n",
            "298/298 [==============================] - 20s 69ms/step - loss: 3.0035 - accuracy: 0.3914\n",
            "Epoch 48/100\n",
            "298/298 [==============================] - 21s 72ms/step - loss: 2.9583 - accuracy: 0.3975\n",
            "Epoch 49/100\n",
            "298/298 [==============================] - 22s 72ms/step - loss: 2.9093 - accuracy: 0.4141\n",
            "Epoch 50/100\n",
            "298/298 [==============================] - 22s 72ms/step - loss: 2.8627 - accuracy: 0.4248\n",
            "Epoch 51/100\n",
            "298/298 [==============================] - 20s 68ms/step - loss: 2.8108 - accuracy: 0.4367\n",
            "Epoch 52/100\n",
            "298/298 [==============================] - 21s 70ms/step - loss: 2.7618 - accuracy: 0.4426\n",
            "Epoch 53/100\n",
            "298/298 [==============================] - 22s 74ms/step - loss: 2.7041 - accuracy: 0.4622\n",
            "Epoch 54/100\n",
            "298/298 [==============================] - 22s 74ms/step - loss: 2.6850 - accuracy: 0.4659\n",
            "Epoch 55/100\n",
            "298/298 [==============================] - 21s 71ms/step - loss: 2.6272 - accuracy: 0.4789\n",
            "Epoch 56/100\n",
            "298/298 [==============================] - 21s 71ms/step - loss: 2.5843 - accuracy: 0.4863\n",
            "Epoch 57/100\n",
            "298/298 [==============================] - 22s 73ms/step - loss: 2.5378 - accuracy: 0.4999\n",
            "Epoch 58/100\n",
            "298/298 [==============================] - 22s 74ms/step - loss: 2.5049 - accuracy: 0.5094\n",
            "Epoch 59/100\n",
            "298/298 [==============================] - 23s 78ms/step - loss: 2.4592 - accuracy: 0.5173\n",
            "Epoch 60/100\n",
            "298/298 [==============================] - 22s 72ms/step - loss: 2.4251 - accuracy: 0.5281\n",
            "Epoch 61/100\n",
            "298/298 [==============================] - 22s 73ms/step - loss: 2.3640 - accuracy: 0.5464\n",
            "Epoch 62/100\n",
            "298/298 [==============================] - 21s 69ms/step - loss: 2.3468 - accuracy: 0.5433\n",
            "Epoch 63/100\n",
            "298/298 [==============================] - 20s 68ms/step - loss: 2.3122 - accuracy: 0.5511\n",
            "Epoch 64/100\n",
            "298/298 [==============================] - 20s 69ms/step - loss: 2.2765 - accuracy: 0.5576\n",
            "Epoch 65/100\n",
            "298/298 [==============================] - 21s 71ms/step - loss: 2.2214 - accuracy: 0.5733\n",
            "Epoch 66/100\n",
            "298/298 [==============================] - 22s 74ms/step - loss: 2.1894 - accuracy: 0.5831\n",
            "Epoch 67/100\n",
            "298/298 [==============================] - 21s 71ms/step - loss: 2.1714 - accuracy: 0.5855\n",
            "Epoch 68/100\n",
            "298/298 [==============================] - 20s 68ms/step - loss: 2.1263 - accuracy: 0.5944\n",
            "Epoch 69/100\n",
            "298/298 [==============================] - 20s 68ms/step - loss: 2.1013 - accuracy: 0.6013\n",
            "Epoch 70/100\n",
            "298/298 [==============================] - 22s 74ms/step - loss: 2.0740 - accuracy: 0.6090\n",
            "Epoch 71/100\n",
            "298/298 [==============================] - 23s 76ms/step - loss: 2.0363 - accuracy: 0.6121\n",
            "Epoch 72/100\n",
            "298/298 [==============================] - 22s 75ms/step - loss: 2.0188 - accuracy: 0.6189\n",
            "Epoch 73/100\n",
            "298/298 [==============================] - 21s 72ms/step - loss: 1.9829 - accuracy: 0.6277\n",
            "Epoch 74/100\n",
            "298/298 [==============================] - 20s 68ms/step - loss: 1.9362 - accuracy: 0.6343\n",
            "Epoch 75/100\n",
            "298/298 [==============================] - 21s 69ms/step - loss: 1.9233 - accuracy: 0.6433\n",
            "Epoch 76/100\n",
            "298/298 [==============================] - 21s 69ms/step - loss: 1.8980 - accuracy: 0.6467\n",
            "Epoch 77/100\n",
            "298/298 [==============================] - 21s 69ms/step - loss: 1.8641 - accuracy: 0.6586\n",
            "Epoch 78/100\n",
            "298/298 [==============================] - 20s 69ms/step - loss: 1.8540 - accuracy: 0.6567\n",
            "Epoch 79/100\n",
            "298/298 [==============================] - 21s 69ms/step - loss: 1.8171 - accuracy: 0.6606\n",
            "Epoch 80/100\n",
            "298/298 [==============================] - 21s 69ms/step - loss: 1.7929 - accuracy: 0.6701\n",
            "Epoch 81/100\n",
            "298/298 [==============================] - 21s 69ms/step - loss: 1.7707 - accuracy: 0.6717\n",
            "Epoch 82/100\n",
            "298/298 [==============================] - 20s 69ms/step - loss: 1.7422 - accuracy: 0.6793\n",
            "Epoch 83/100\n",
            "298/298 [==============================] - 21s 69ms/step - loss: 1.7194 - accuracy: 0.6876\n",
            "Epoch 84/100\n",
            "298/298 [==============================] - 20s 68ms/step - loss: 1.6967 - accuracy: 0.6909\n",
            "Epoch 85/100\n",
            "298/298 [==============================] - 20s 69ms/step - loss: 1.6693 - accuracy: 0.6979\n",
            "Epoch 86/100\n",
            "298/298 [==============================] - 24s 80ms/step - loss: 1.6609 - accuracy: 0.6970\n",
            "Epoch 87/100\n",
            "298/298 [==============================] - 30s 102ms/step - loss: 1.6301 - accuracy: 0.7021\n",
            "Epoch 88/100\n",
            "298/298 [==============================] - 35s 119ms/step - loss: 1.6179 - accuracy: 0.7040\n",
            "Epoch 89/100\n",
            "298/298 [==============================] - 30s 101ms/step - loss: 1.5973 - accuracy: 0.7105\n",
            "Epoch 90/100\n",
            "298/298 [==============================] - 16s 53ms/step - loss: 1.5795 - accuracy: 0.7143\n",
            "Epoch 91/100\n",
            "298/298 [==============================] - 22s 73ms/step - loss: 1.5662 - accuracy: 0.7177\n",
            "Epoch 92/100\n",
            "298/298 [==============================] - 41s 138ms/step - loss: 1.5380 - accuracy: 0.7230\n",
            "Epoch 93/100\n",
            "298/298 [==============================] - 39s 130ms/step - loss: 1.5170 - accuracy: 0.7306\n",
            "Epoch 94/100\n",
            "298/298 [==============================] - 43s 144ms/step - loss: 1.5025 - accuracy: 0.7274\n",
            "Epoch 95/100\n",
            "298/298 [==============================] - 51s 171ms/step - loss: 1.4759 - accuracy: 0.7360\n",
            "Epoch 96/100\n",
            "298/298 [==============================] - 49s 165ms/step - loss: 1.4695 - accuracy: 0.7372\n",
            "Epoch 97/100\n",
            "298/298 [==============================] - 52s 174ms/step - loss: 1.4557 - accuracy: 0.7416\n",
            "Epoch 98/100\n",
            "298/298 [==============================] - 55s 185ms/step - loss: 1.4318 - accuracy: 0.7472\n",
            "Epoch 99/100\n",
            "298/298 [==============================] - 56s 188ms/step - loss: 1.3953 - accuracy: 0.7518\n",
            "Epoch 100/100\n",
            "298/298 [==============================] - 52s 173ms/step - loss: 1.3988 - accuracy: 0.7529\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(predictors, label, epochs=100, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "colab_type": "code",
        "id": "6Vc6PHgxa6Hm",
        "outputId": "46c4b22e-1b44-4818-c34d-10a56fe889c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The man who taught ML had no quarrel from forty years ago as ' ' ' ' one more ' to having each step who clasps them are toffile right ' ' ' of family ' i us toffile to care ' ' the night ' ' ' his father glacier round as that slippery\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"The man who taught ML\"\n",
        "next_words = 50\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list,verbose=0), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\t# print(predicted)\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NLP-Week4-Exercise-Shakespeare-Question.ipynb",
      "provenance": [],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
